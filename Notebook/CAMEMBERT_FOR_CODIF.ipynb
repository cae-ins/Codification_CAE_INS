{"cells":[{"cell_type":"markdown","metadata":{"id":"JDSnLitbFrzd"},"source":["# Importation des bibliothèques "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7712,"status":"ok","timestamp":1678791231665,"user":{"displayName":"Franck MIGONE","userId":"02633380724188141696"},"user_tz":0},"id":"RBLPhZ24EG_A","outputId":"b9da2fc1-e062-423c-c6d8-a5103fac75ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.26.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsAx7j62ExLT","outputId":"c8af3ef1-b857-4356-e9a5-4f703608ae67","executionInfo":{"status":"ok","timestamp":1663336609309,"user_tz":0,"elapsed":3888,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 5.0 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}],"source":["%pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYORTjzyEyMJ","colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["8159c023019644139af687d1512343e8","3c631c96878a478da6ce79399cbb5095","653c364fed1f4d269754c8bd3a37f5ee","f0ea8ff1338b480c9ca7f5f0898dfad1","b4b04cc37a6348fd9c00ca6fe4f40580","3db0a8f0c45a4f27a358e2aac4d8d8e2","68a8bad307f54985ae032a3076551256","e91c4e6f42284ee6aeff00d896154519","e413a3f9538a491591219857a66184f9","ae42b0bcd6164425a3611431efc8158e","d60d06f04dc34ce8831d135130dcd14d"]},"executionInfo":{"status":"ok","timestamp":1663336623473,"user_tz":0,"elapsed":9336,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"}},"outputId":"d4aaa320-4b26-4ebb-f5f1-672fd3fdf5b9"},"outputs":[{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"stream","name":"stdout","text":["Moving 0 files to the new cache system\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8159c023019644139af687d1512343e8"}},"metadata":{}}],"source":["import torch\n","import seaborn\n","import os\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW\n","# Chargement du GPU\n","device = torch.device(\"cuda\")"]},{"cell_type":"markdown","metadata":{"id":"5ZPKYmDPF3pJ"},"source":["# Importation du jeu de données et Encodage"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9472,"status":"ok","timestamp":1663337193835,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"},"user_tz":0},"id":"cSuqv2cvFnk5","outputId":"72d9e061-5b70-4e68-8e30-02944686df89"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["         NCC                                     RAISON SOCIALE  \\\n","0   9815068B                             BARRY CALLEBAUT NEGOCE   \n","1   9606123E                            ORANGE COTE D'IVOIRE SA   \n","2   7603142C                             TOTAL COTE D'IVOIRE SA   \n","3   9818944H  SOCIETE DE DISTRIBUTION DE TOUTES MARCHANDISES...   \n","4   0100690X                          VIVO ENERGY CÔTE D'IVOIRE   \n","5   8607802Q                                CARGILL WEST AFRICA   \n","6   9606689W                                  MTN COTE D'IVOIRE   \n","7   5011806N    SOCIETE IVOIRIENNE DE PROMOTION DE SUPERMARCHES   \n","8   0179308E                         IVOIRIENNE DE DISTRIBUTION   \n","9   0521319F                   ATLANTIQUE TELECOM COTE D'IVOIRE   \n","10  0913981R                        SOCIETE DES MINES DE TONGON   \n","11  9000473M         COMPAGNIE DE DISTRIBUTION DE COTE D'IVOIRE   \n","12  1105355G                             NKSUCDEN COTE D'IVOIRE   \n","13  9800049W                                  OUTSPAN IVOIRE SA   \n","14  9800049W                                  OUTSPAN IVOIRE SA   \n","15  0815951Y           SOCIETE DE COMMERCIALISATION CAFE- CACAO   \n","16  0100699D                            LIBYA OIL COTE D'IVOIRE   \n","17  9511998U                        TOUTON NEGOCE COTE D'IVOIRE   \n","18  7602349S  SOCIETE NATIONALE D'OPERATIONS PETROLIERES DE ...   \n","19  0100717M                             LABOREX -COTE D IVOIRE   \n","\n","                SIGLE                                ACTIVITE PRINCIPALE  \\\n","0             BCN  SA           USINAGHE ET EXPORTATIO DE FEVES DE CACAO   \n","1                 NaN  Télécommunications, fournisseurs d'accès internet   \n","2            TOTAL CI                                       DISTRIBUTION   \n","3             SDTM-CI                     IMPORT ET EXPORT DIISTRIBUTION   \n","4      VIVO ENERGY CI                DISTRIBUTION DE PRODUITS PETROLIERS   \n","5                 CWA                             NEGOCEDE CAFE ET CACAO   \n","6                 NaN  Télécommunications, fournisseurs d'accès internet   \n","7             PROSUMA                                GRANDE DISTRIBUTION   \n","8                IDIS                               COMMERCE DE BOISSONS   \n","9                 NaN  Télécommunications, fournisseurs d'accès internet   \n","10         TONGON S.A                               EXPLOITATION MINIERE   \n","11               CDCI                    Commerce de gros non spécialisé   \n","12         SUCDEN -CI                                   NGOCE CAFE CACAO   \n","13  OUTSPAN IVOIRE SA  Commerce de gros de produits agricoles bruts e...   \n","14  OUTSPAN IVOIRE SA  Commerce de gros de produits agricoles bruts e...   \n","15              S.3.C  COMMERCIALISATION DE PRODUITS AGRICOLES BRUTS ...   \n","16             OLIBYA                DISTRIBUTION DE PRODUITS PETROLIERS   \n","17               TNCI                                                  0   \n","18            PETROCI            RECHERCHE ET PRODUCTION D HYDROCARBURES   \n","19        UBIPHARM CI              GROSSISTE DE PRODUITS PHARMACEUTIQUES   \n","\n","   BRANCHE D'ACTIVITE (CAPCN*)  \\\n","0                       G34002   \n","1                       J37004   \n","2                       G34003   \n","3                       G34003   \n","4                       G34004   \n","5                       G34002   \n","6                       J37004   \n","7                       G34003   \n","8                       G34002   \n","9                       J37004   \n","10                      B07004   \n","11                      G34003   \n","12                      G34002   \n","13                      G34002   \n","14                      G34002   \n","15                      G34002   \n","16                      G34003   \n","17                      G34002   \n","18                      G34003   \n","19                      G34003   \n","\n","                          LIBELLE D'ACTIVITE DETAILLE CODE PRODUIT (CAPCN*)  \\\n","0                                      VENTE DE CACAO             G34002000   \n","1                                  TELECOMMUNICATIONS             J37004001   \n","2   COMMERCE DE GROS DE CARBURANTS ET DE COMBUSTIBLES             G34003002   \n","3                                      VENTES LOCALES             G34003002   \n","4                 DISTRIBUTION DE PRODUITS PETROLIERS             G34004000   \n","5                                    NEGOCES DE CACAO             G34002000   \n","6                             VENTES SERVICES TELECOM             J37004001   \n","7                                 GRANDE DISTRIBUTION             G34003002   \n","8                                COMMERCE DE BOISSONS             G34002000   \n","9                                   Télephonie mobile             J37004001   \n","10          EXTRACTION DE MINERAIS DE METAUX PRECIEUX             B07004001   \n","11                              VENTE DE MARCHANDISES             G34003002   \n","12                              VENTE DE MARCHANDISES             G34002000   \n","13                              VENTE EXPORT DE CACAO             G34002000   \n","14                              VENTE LOCALE DE CACAO             G34002000   \n","15             COMMERCIALISATION DE PRODUITS AGRICOLE             G34002000   \n","16        COMMERCE DE GROS CARBURANTS ET COMBUSTIBLES             G34003002   \n","17                       VENTE A L'EXPORT CAFE -CACAO             G34002000   \n","18     VENTE DE BUTANE CARBURANT BITUME ET LUBRIFIANT             G34003002   \n","19                           PRODUITS PHARMACEUTIQUES             G34003002   \n","\n","    NOMBRE_OCCUR  class                                     libelle_concat  \n","0           2258     29  USINAGHE ET EXPORTATIO DE FEVES DE CACAO plus ...  \n","1            189     42  Télécommunications, fournisseurs d'accès inter...  \n","2            566     31  DISTRIBUTION plus précisément COMMERCE DE GROS...  \n","3            566     31  IMPORT ET EXPORT DIISTRIBUTION plus précisémen...  \n","4          11443     32  DISTRIBUTION DE PRODUITS PETROLIERS plus préci...  \n","5           2258     29  NEGOCEDE CAFE ET CACAO plus précisément NEGOCE...  \n","6            189     42  Télécommunications, fournisseurs d'accès inter...  \n","7            566     31  GRANDE DISTRIBUTION plus précisément GRANDE DI...  \n","8           2258     29  COMMERCE DE BOISSONS plus précisément COMMERCE...  \n","9            189     42  Télécommunications, fournisseurs d'accès inter...  \n","10            47      3  EXPLOITATION MINIERE plus précisément EXTRACTI...  \n","11           566     31  Commerce de gros non spécialisé plus préciséme...  \n","12          2258     29  NGOCE CAFE CACAO plus précisément VENTE DE MAR...  \n","13          2258     29  Commerce de gros de produits agricoles bruts e...  \n","14          2258     29  Commerce de gros de produits agricoles bruts e...  \n","15          2258     29  COMMERCIALISATION DE PRODUITS AGRICOLES BRUTS ...  \n","16           566     31  DISTRIBUTION DE PRODUITS PETROLIERS plus préci...  \n","17          2258     29    0 plus précisément VENTE A L'EXPORT CAFE -CACAO  \n","18           566     31  RECHERCHE ET PRODUCTION D HYDROCARBURES plus p...  \n","19           566     31  GROSSISTE DE PRODUITS PHARMACEUTIQUES plus pré...  "],"text/html":["\n","  <div id=\"df-3bdd4cd2-8555-4c53-9b04-4578922a8eb5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>NCC</th>\n","      <th>RAISON SOCIALE</th>\n","      <th>SIGLE</th>\n","      <th>ACTIVITE PRINCIPALE</th>\n","      <th>BRANCHE D'ACTIVITE (CAPCN*)</th>\n","      <th>LIBELLE D'ACTIVITE DETAILLE</th>\n","      <th>CODE PRODUIT (CAPCN*)</th>\n","      <th>NOMBRE_OCCUR</th>\n","      <th>class</th>\n","      <th>libelle_concat</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9815068B</td>\n","      <td>BARRY CALLEBAUT NEGOCE</td>\n","      <td>BCN  SA</td>\n","      <td>USINAGHE ET EXPORTATIO DE FEVES DE CACAO</td>\n","      <td>G34002</td>\n","      <td>VENTE DE CACAO</td>\n","      <td>G34002000</td>\n","      <td>2258</td>\n","      <td>29</td>\n","      <td>USINAGHE ET EXPORTATIO DE FEVES DE CACAO plus ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9606123E</td>\n","      <td>ORANGE COTE D'IVOIRE SA</td>\n","      <td>NaN</td>\n","      <td>Télécommunications, fournisseurs d'accès internet</td>\n","      <td>J37004</td>\n","      <td>TELECOMMUNICATIONS</td>\n","      <td>J37004001</td>\n","      <td>189</td>\n","      <td>42</td>\n","      <td>Télécommunications, fournisseurs d'accès inter...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7603142C</td>\n","      <td>TOTAL COTE D'IVOIRE SA</td>\n","      <td>TOTAL CI</td>\n","      <td>DISTRIBUTION</td>\n","      <td>G34003</td>\n","      <td>COMMERCE DE GROS DE CARBURANTS ET DE COMBUSTIBLES</td>\n","      <td>G34003002</td>\n","      <td>566</td>\n","      <td>31</td>\n","      <td>DISTRIBUTION plus précisément COMMERCE DE GROS...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9818944H</td>\n","      <td>SOCIETE DE DISTRIBUTION DE TOUTES MARCHANDISES...</td>\n","      <td>SDTM-CI</td>\n","      <td>IMPORT ET EXPORT DIISTRIBUTION</td>\n","      <td>G34003</td>\n","      <td>VENTES LOCALES</td>\n","      <td>G34003002</td>\n","      <td>566</td>\n","      <td>31</td>\n","      <td>IMPORT ET EXPORT DIISTRIBUTION plus précisémen...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0100690X</td>\n","      <td>VIVO ENERGY CÔTE D'IVOIRE</td>\n","      <td>VIVO ENERGY CI</td>\n","      <td>DISTRIBUTION DE PRODUITS PETROLIERS</td>\n","      <td>G34004</td>\n","      <td>DISTRIBUTION DE PRODUITS PETROLIERS</td>\n","      <td>G34004000</td>\n","      <td>11443</td>\n","      <td>32</td>\n","      <td>DISTRIBUTION DE PRODUITS PETROLIERS plus préci...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>8607802Q</td>\n","      <td>CARGILL WEST AFRICA</td>\n","      <td>CWA</td>\n","      <td>NEGOCEDE CAFE ET CACAO</td>\n","      <td>G34002</td>\n","      <td>NEGOCES DE CACAO</td>\n","      <td>G34002000</td>\n","      <td>2258</td>\n","      <td>29</td>\n","      <td>NEGOCEDE CAFE ET CACAO plus précisément NEGOCE...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>9606689W</td>\n","      <td>MTN COTE D'IVOIRE</td>\n","      <td>NaN</td>\n","      <td>Télécommunications, fournisseurs d'accès internet</td>\n","      <td>J37004</td>\n","      <td>VENTES SERVICES TELECOM</td>\n","      <td>J37004001</td>\n","      <td>189</td>\n","      <td>42</td>\n","      <td>Télécommunications, fournisseurs d'accès inter...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>5011806N</td>\n","      <td>SOCIETE IVOIRIENNE DE PROMOTION DE SUPERMARCHES</td>\n","      <td>PROSUMA</td>\n","      <td>GRANDE DISTRIBUTION</td>\n","      <td>G34003</td>\n","      <td>GRANDE DISTRIBUTION</td>\n","      <td>G34003002</td>\n","      <td>566</td>\n","      <td>31</td>\n","      <td>GRANDE DISTRIBUTION plus précisément GRANDE DI...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0179308E</td>\n","      <td>IVOIRIENNE DE DISTRIBUTION</td>\n","      <td>IDIS</td>\n","      <td>COMMERCE DE BOISSONS</td>\n","      <td>G34002</td>\n","      <td>COMMERCE DE BOISSONS</td>\n","      <td>G34002000</td>\n","      <td>2258</td>\n","      <td>29</td>\n","      <td>COMMERCE DE BOISSONS plus précisément COMMERCE...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0521319F</td>\n","      <td>ATLANTIQUE TELECOM COTE D'IVOIRE</td>\n","      <td>NaN</td>\n","      <td>Télécommunications, fournisseurs d'accès internet</td>\n","      <td>J37004</td>\n","      <td>Télephonie mobile</td>\n","      <td>J37004001</td>\n","      <td>189</td>\n","      <td>42</td>\n","      <td>Télécommunications, fournisseurs d'accès inter...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0913981R</td>\n","      <td>SOCIETE DES MINES DE TONGON</td>\n","      <td>TONGON S.A</td>\n","      <td>EXPLOITATION MINIERE</td>\n","      <td>B07004</td>\n","      <td>EXTRACTION DE MINERAIS DE METAUX PRECIEUX</td>\n","      <td>B07004001</td>\n","      <td>47</td>\n","      <td>3</td>\n","      <td>EXPLOITATION MINIERE plus précisément EXTRACTI...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>9000473M</td>\n","      <td>COMPAGNIE DE DISTRIBUTION DE COTE D'IVOIRE</td>\n","      <td>CDCI</td>\n","      <td>Commerce de gros non spécialisé</td>\n","      <td>G34003</td>\n","      <td>VENTE DE MARCHANDISES</td>\n","      <td>G34003002</td>\n","      <td>566</td>\n","      <td>31</td>\n","      <td>Commerce de gros non spécialisé plus préciséme...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1105355G</td>\n","      <td>NKSUCDEN COTE D'IVOIRE</td>\n","      <td>SUCDEN -CI</td>\n","      <td>NGOCE CAFE CACAO</td>\n","      <td>G34002</td>\n","      <td>VENTE DE MARCHANDISES</td>\n","      <td>G34002000</td>\n","      <td>2258</td>\n","      <td>29</td>\n","      <td>NGOCE CAFE CACAO plus précisément VENTE DE MAR...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>9800049W</td>\n","      <td>OUTSPAN IVOIRE SA</td>\n","      <td>OUTSPAN IVOIRE SA</td>\n","      <td>Commerce de gros de produits agricoles bruts e...</td>\n","      <td>G34002</td>\n","      <td>VENTE EXPORT DE CACAO</td>\n","      <td>G34002000</td>\n","      <td>2258</td>\n","      <td>29</td>\n","      <td>Commerce de gros de produits agricoles bruts e...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>9800049W</td>\n","      <td>OUTSPAN IVOIRE SA</td>\n","      <td>OUTSPAN IVOIRE SA</td>\n","      <td>Commerce de gros de produits agricoles bruts e...</td>\n","      <td>G34002</td>\n","      <td>VENTE LOCALE DE CACAO</td>\n","      <td>G34002000</td>\n","      <td>2258</td>\n","      <td>29</td>\n","      <td>Commerce de gros de produits agricoles bruts e...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0815951Y</td>\n","      <td>SOCIETE DE COMMERCIALISATION CAFE- CACAO</td>\n","      <td>S.3.C</td>\n","      <td>COMMERCIALISATION DE PRODUITS AGRICOLES BRUTS ...</td>\n","      <td>G34002</td>\n","      <td>COMMERCIALISATION DE PRODUITS AGRICOLE</td>\n","      <td>G34002000</td>\n","      <td>2258</td>\n","      <td>29</td>\n","      <td>COMMERCIALISATION DE PRODUITS AGRICOLES BRUTS ...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0100699D</td>\n","      <td>LIBYA OIL COTE D'IVOIRE</td>\n","      <td>OLIBYA</td>\n","      <td>DISTRIBUTION DE PRODUITS PETROLIERS</td>\n","      <td>G34003</td>\n","      <td>COMMERCE DE GROS CARBURANTS ET COMBUSTIBLES</td>\n","      <td>G34003002</td>\n","      <td>566</td>\n","      <td>31</td>\n","      <td>DISTRIBUTION DE PRODUITS PETROLIERS plus préci...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>9511998U</td>\n","      <td>TOUTON NEGOCE COTE D'IVOIRE</td>\n","      <td>TNCI</td>\n","      <td>0</td>\n","      <td>G34002</td>\n","      <td>VENTE A L'EXPORT CAFE -CACAO</td>\n","      <td>G34002000</td>\n","      <td>2258</td>\n","      <td>29</td>\n","      <td>0 plus précisément VENTE A L'EXPORT CAFE -CACAO</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>7602349S</td>\n","      <td>SOCIETE NATIONALE D'OPERATIONS PETROLIERES DE ...</td>\n","      <td>PETROCI</td>\n","      <td>RECHERCHE ET PRODUCTION D HYDROCARBURES</td>\n","      <td>G34003</td>\n","      <td>VENTE DE BUTANE CARBURANT BITUME ET LUBRIFIANT</td>\n","      <td>G34003002</td>\n","      <td>566</td>\n","      <td>31</td>\n","      <td>RECHERCHE ET PRODUCTION D HYDROCARBURES plus p...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0100717M</td>\n","      <td>LABOREX -COTE D IVOIRE</td>\n","      <td>UBIPHARM CI</td>\n","      <td>GROSSISTE DE PRODUITS PHARMACEUTIQUES</td>\n","      <td>G34003</td>\n","      <td>PRODUITS PHARMACEUTIQUES</td>\n","      <td>G34003002</td>\n","      <td>566</td>\n","      <td>31</td>\n","      <td>GROSSISTE DE PRODUITS PHARMACEUTIQUES plus pré...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3bdd4cd2-8555-4c53-9b04-4578922a8eb5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3bdd4cd2-8555-4c53-9b04-4578922a8eb5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3bdd4cd2-8555-4c53-9b04-4578922a8eb5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["#Chargement du jeu de données\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","df = pd.read_excel(\"/content/drive/MyDrive/codif_compta.xlsx\")\n","df.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlVmuI-FGVIA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663337946126,"user_tz":0,"elapsed":298,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"}},"outputId":"2d0c15a0-e65f-4e57-be9d-3d949c696dd5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}],"source":["df = df[['libelle_concat', 'class']]\n","df['Libelle'] = df['libelle_concat'].astype(str)\n","df['CODE_NEW_3_to_2'] = df['class'].astype(int)\n","#df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWMSg2tpIAVy"},"outputs":[],"source":["Libelle = df['Libelle'].values.tolist()\n","CODE = df['CODE_NEW_3_to_2'].values.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mo0Ys9ixHEF-"},"outputs":[],"source":["num_labels = len(df.iloc[:,1].unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZZJE9arzcdg"},"outputs":[],"source":["# split train dataset into train, validation and test sets\n","train_Libelle, temp_Libelle, train_CODE, temp_CODE = train_test_split(df['Libelle'], df['CODE_NEW_3_to_2'], random_state=2011, test_size=0.3)\n","val_Libelle, test_Libelle, val_CODE, test_CODE = train_test_split(temp_Libelle, temp_CODE, random_state = 2018, test_size = 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clDZnqE30f65","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["ee543c72d7314fc9bc8dec6691612001","e1823470ecfa447fa1d868af66d77b52","60d5e58dc0e44438aa39f5f96c887e99","ca24abdaf11b49a59f59d962ece65e4d","179b99aa78f64367aa3c757dbd4c23ae","cc5fff94e55243c6a57639666e2580b4","4cd1583344f9478490e18c3f5a5ba798","09a80f2c4c094d4c81c91b367ff88e52","6db6437e3de0446281646d278faf394d","2b3d57ce35374253aa229c2e3b00b7ee","86e91b31e8f542319d9ad55499fb5037","d5bd3cc371294993829825d47009a2bc","4be677ff599c4d6eaf104a3ada44f858","ee970b3994f54db8bb4c94e63ea406aa","b1173db4c4ac4b1386a2f62dbc86d552","f1be8ef26a394ec5a1487126b33423f9","65ac6bd912614a13af485d35c4069bda","383e5f8e7444452eba1f5d59aa18e87c","f700458ad5e147eb974be7c419880698","f4ee898f344c4adb90bb5b42aa1310f8","2e47316a128645e28ee722870f02e6b5","4114439fdf9c4ef29ab72b5931affa9d"]},"executionInfo":{"status":"ok","timestamp":1663337964081,"user_tz":0,"elapsed":1551,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"}},"outputId":"48487f6b-6d56-4133-dcd4-aae5923eee6f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/811k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee543c72d7314fc9bc8dec6691612001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5bd3cc371294993829825d47009a2bc"}},"metadata":{}}],"source":["# On charge l'objet \"tokenizer\"de camemBERT qui va servir a encoder\n","# 'camebert-base' est la version de camembert qu'on choisit d'utiliser\n","# 'do_lower_case' à True pour qu'on passe tout en miniscule\n","tokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1663337966817,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"},"user_tz":0},"id":"UDl19s3iI1LW","outputId":"462f589a-cde1-440c-8428-a09d7d88d4d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f4713b3e390>"]},"metadata":{},"execution_count":12},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# Distribution de la longueur des phrases\n","# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in train_Libelle]\n","\n","pd.Series(seq_len).hist(bins = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7976,"status":"ok","timestamp":1663338003266,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"},"user_tz":0},"id":"rwe25EimHaWy","outputId":"31c1ad26-e408-4e06-a2f5-e8f5015945c6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2306: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["# Tokenisation des phases dans nos différents jeu de données avec max_length = 6 pour le padding \n","# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(train_Libelle,max_length = 30, pad_to_max_length=True,truncation=True,\n","                                           return_tensors = 'pt')\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(val_Libelle,max_length = 6,pad_to_max_length=True,truncation=True,\n","                                         return_tensors = 'pt')\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(test_Libelle,max_length = 4,pad_to_max_length=True,truncation=True,\n","                                            return_tensors = 'pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"De2ElcTT4_5z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663338005100,"user_tz":0,"elapsed":261,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"}},"outputId":"4aae49f3-d0eb-4fbe-c727-85d8cf1c300e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if sys.path[0] == '':\n"]}],"source":["## convert lists to tensors / convertion des sequences de données en tenseurs pour optimiser les calculs\n","\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_CODE.tolist()) - 1\n","\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_CODE.tolist()) - 1\n","\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_CODE.tolist()) - 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvTrmgQc6DOd"},"outputs":[],"source":["\"\"\" Nous allons maintenant créer des chargeurs de données pour le train et l'ensemble de validation. \n","    Ces chargeurs de données transmettront des lots de données de train et de données de validation en entrée du modèle pendant la phase de formation.\n","\"\"\"\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"n3nTiLg-LdR8"},"source":["# Chargement du modèle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["a6ca55f53312472f8694597aecb8cebb","eb1e5856c46c4c989b0e268642a059a8","e8b5c2d856744bc5b43f3da83e8c994c","e2d13acb02ff449896b020e8e850ee41","1b984e85f71c4a82ae4a8598c74e56b5","d1a68827591f463e9ad448393548bdba","af04478534674808941cc2db64456fef","14fcbf82be2f478b866272f3019395fa","3368e92c19d542b7b0c4aef38f06069b","855fa8e8b1804b7a825b7a0409c86eb6","3720e112425b490b9dbf6f9e70158ddd"]},"executionInfo":{"elapsed":16284,"status":"ok","timestamp":1663338029936,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"},"user_tz":0},"id":"q6G8WDdvJ8cR","outputId":"7d8d6b14-8782-4875-8d87-b92ffe15e83d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6ca55f53312472f8694597aecb8cebb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# On la version pre-entrainee de camemBERT 'base'\n","model = CamembertForSequenceClassification.from_pretrained(\n","    'camembert-base',\n","    num_labels = num_labels)\n","\n","# push the model to GPU\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"eOdbVTRyLnx5"},"source":["# Hyperparamètres"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1663338041579,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"},"user_tz":0},"id":"VhFgfcTyLY_6","outputId":"2b0173cc-2f65-4b17-d0a8-646aa14527ff"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2.5e-5, # Learning Rate\n","                  eps = 1e-8 # Epsilon\n",")\n","epochs = 30"]},{"cell_type":"markdown","metadata":{"id":"eY3fEsQJv1Vq"},"source":["# Desequilibre des classes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1663338049359,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"},"user_tz":0},"id":"-4E7ei3Vv7-R","outputId":"e8b64a4a-da40-46eb-a955-a369dd651139"},"outputs":[{"output_type":"stream","name":"stdout","text":["Class Weights: {1: 6.09982174688057, 2: 4.786013986013986, 3: 11.521885521885523, 4: 11.521885521885523, 5: 11.11038961038961, 6: 5.869639794168096, 7: 11.965034965034965, 8: 14.813852813852813, 9: 1.2057787174066243, 10: 3.988344988344988, 11: 9.721590909090908, 12: 5.2727272727272725, 13: 15.554545454545455, 14: 4.574866310160428, 15: 1.229608336327704, 16: 9.721590909090908, 17: 9.149732620320856, 18: 5.457735247208931, 19: 9.721590909090908, 20: 1.2854996243425996, 21: 6.481060606060606, 22: 1.8628198149156232, 23: 5.184848484848485, 24: 3.049910873440285, 25: 0.2106234997230258, 26: 0.5410276679841898, 27: 0.5210902999847723, 28: 1.0801767676767677, 29: 0.1962718669343275, 30: 4.860795454545454, 31: 0.803852478271083, 32: 0.03861605127742168, 33: 4.261519302615193, 34: 4.860795454545454, 35: 9.149732620320856, 36: 1.2057787174066243, 37: 0.7217886521830837, 38: 0.4720651124293006, 39: 0.6576974822217951, 40: 7.777272727272727, 41: 4.508563899868248, 42: 2.393006993006993, 43: 8.186602870813397, 44: 0.7875719217491369, 45: 7.587583148558759, 46: 3.4185814185814185, 47: 1.0369696969696969, 48: 1.8968957871396896, 49: 1.829946524064171, 50: 5.2727272727272725, 51: 1.0953905249679898, 52: 2.17546090273363, 53: 0.307402084081926, 54: 0.9484478935698448, 55: 0.8913779630112009, 56: 2.828099173553719, 57: 1.6547388781431334, 58: 4.508563899868248, 59: 2.2707365627073655, 60: 1.7187343043696635, 61: 1.4605206999573197, 62: 3.617336152219873, 63: 1.0035190615835776, 64: 0.9601571268237935, 65: 0.5391523554435166, 66: 4.1478787878787875, 67: 0.8430647942843065, 68: 0.8407862407862408, 69: 4.203931203931204, 70: 10.36969696969697, 71: 5.555194805194805, 72: 3.7937915742793793, 73: 6.6189555125725335, 74: 4.320707070707071, 75: 5.760942760942761, 76: 14.140495867768594, 77: 9.149732620320856}\n"]}],"source":["# il y a un déséquilibre dans les classes c-a-d il y a assez de spam que de non spam il faut donc en tenir compte\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_weights = compute_class_weight(class_weight='balanced', classes = np.unique(train_CODE), y= train_CODE)\n","class_weights = dict(zip(np.unique(train_CODE), class_weights))\n","\n","d = list(class_weights.values())\n","d.append(0)\n","\n","print(\"Class Weights:\",class_weights)\n","\n","# converting list of class weights to a tensor\n","weights= torch.tensor(list(class_weights.values()),dtype=torch.float)\n","\n","weights = weights.to(device)\n","\n","# define the loss function\n","cross_entropy  = nn.NLLLoss(weight=weights)"]},{"cell_type":"markdown","metadata":{"id":"x9nPCEe_MA8V"},"source":["# Entrainement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCIHkqtc-4IV"},"outputs":[],"source":["# fonctions pour ajuster le modele\n","\n","# function to train the model\n","def train():\n","\n","  model.train()\n","  total_loss, total_accuracy = 0, 0\n","  # empty list to save model predictions\n","  total_preds=[]\n","  # iterate over batches\n","  for step,batch in enumerate(train_dataloader):\n","\n","    # progress update after every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [r.to(device) for r in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # clear previously calculated gradients \n","    model.zero_grad()        \n","\n","    # get model predictions for the current batch\n","    preds = model(sent_id, mask)\n","    _, logits = model(sent_id, \n","                             token_type_ids=None, \n","                             attention_mask=mask, \n","                             labels=labels,return_dict=False)\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(logits, labels)\n","\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # update parameters\n","    optimizer.step()\n","\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds = logits\n","    preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  #returns the loss and predictions\n","  return avg_loss, total_preds\n"," \n","    \n","  #return "]},{"cell_type":"markdown","metadata":{"id":"itrudbMUNJHs"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWfqs1mpNK84"},"outputs":[],"source":["# function for evaluating the model\n","def evaluate():\n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save the model predictions\n","  total_preds = []\n","\n","  # iterate over batches\n","  for step,batch in enumerate(val_dataloader):\n","    \n","    # Progress update every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      \n","      # Calculate elapsed time in minutes.\n","      #elapsed = format_time(time.time()-t0)\n","            \n","      # Report progress.\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [t.to(device) for t in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","      \n","      # model predictions\n","      preds = model(sent_id, mask)\n","      #preds = logits\n","      _, logits = model(sent_id, \n","                             token_type_ids=None, \n","                             attention_mask=mask, \n","                             labels=labels,return_dict=False)\n","\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(logits,labels)\n","\n","      total_loss = total_loss + loss.item()\n","      preds = logits\n","\n","      preds = preds.detach().cpu().numpy()\n","\n","      total_preds.append(preds)\n","\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader) \n","\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  return avg_loss, total_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5980080,"status":"ok","timestamp":1663344047813,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"},"user_tz":0},"id":"veUiB5zgRLaH","outputId":"a056191e-bd8c-4ea1-a772-18af706ae107"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -1.554\n","Validation Loss: -4.224\n","\n"," Epoch 2 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -7.956\n","Validation Loss: -11.527\n","\n"," Epoch 3 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -14.771\n","Validation Loss: -17.565\n","\n"," Epoch 4 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -20.557\n","Validation Loss: -23.061\n","\n"," Epoch 5 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -26.072\n","Validation Loss: -28.451\n","\n"," Epoch 6 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -31.378\n","Validation Loss: -33.718\n","\n"," Epoch 7 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -37.011\n","Validation Loss: -39.039\n","\n"," Epoch 8 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -42.154\n","Validation Loss: -44.288\n","\n"," Epoch 9 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -47.388\n","Validation Loss: -49.566\n","\n"," Epoch 10 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -52.897\n","Validation Loss: -54.884\n","\n"," Epoch 11 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -58.320\n","Validation Loss: -60.147\n","\n"," Epoch 12 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -63.506\n","Validation Loss: -65.385\n","\n"," Epoch 13 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -68.856\n","Validation Loss: -70.667\n","\n"," Epoch 14 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -73.971\n","Validation Loss: -75.941\n","\n"," Epoch 15 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -79.912\n","Validation Loss: -81.253\n","\n"," Epoch 16 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -84.918\n","Validation Loss: -86.487\n","\n"," Epoch 17 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -90.241\n","Validation Loss: -91.714\n","\n"," Epoch 18 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -95.442\n","Validation Loss: -96.975\n","\n"," Epoch 19 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -100.885\n","Validation Loss: -102.241\n","\n"," Epoch 20 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -106.019\n","Validation Loss: -107.530\n","\n"," Epoch 21 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -111.128\n","Validation Loss: -112.804\n","\n"," Epoch 22 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -116.577\n","Validation Loss: -118.084\n","\n"," Epoch 23 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -122.343\n","Validation Loss: -123.363\n","\n"," Epoch 24 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -127.922\n","Validation Loss: -128.667\n","\n"," Epoch 25 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -133.353\n","Validation Loss: -133.953\n","\n"," Epoch 26 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -138.294\n","Validation Loss: -139.212\n","\n"," Epoch 27 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -143.569\n","Validation Loss: -144.457\n","\n"," Epoch 28 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -148.760\n","Validation Loss: -149.707\n","\n"," Epoch 29 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -153.653\n","Validation Loss: -154.946\n","\n"," Epoch 30 / 30\n","  Batch    50  of    749.\n","  Batch   100  of    749.\n","  Batch   150  of    749.\n","  Batch   200  of    749.\n","  Batch   250  of    749.\n","  Batch   300  of    749.\n","  Batch   350  of    749.\n","  Batch   400  of    749.\n","  Batch   450  of    749.\n","  Batch   500  of    749.\n","  Batch   550  of    749.\n","  Batch   600  of    749.\n","  Batch   650  of    749.\n","  Batch   700  of    749.\n","\n","Evaluating...\n","  Batch    50  of    161.\n","  Batch   100  of    161.\n","  Batch   150  of    161.\n","\n","Training Loss: -159.334\n","Validation Loss: -160.230\n"]}],"source":["\n","# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = train()\n","    \n","#evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"]},{"cell_type":"markdown","metadata":{"id":"XxuSf8JqAhsw"},"source":["# Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":676,"status":"ok","timestamp":1663344518012,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"},"user_tz":0},"id":"hGfP21b8BHsu","outputId":"69f38b90-1df2-400e-9abf-de38aed86909"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":22}],"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQBvtjN5Aozz"},"outputs":[],"source":["# get predictions for test data\n","with torch.no_grad():\n","  models = model(test_seq.to(device), test_mask.to(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1663344520782,"user":{"displayName":"Shmoël Sre","userId":"16093550214089889904"},"user_tz":0},"id":"_S2FHuU5AuNe","outputId":"367ba142-06bf-4c1d-d950-75556c0e45c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00        12\n","           1       0.00      0.00      0.00        18\n","           2       0.00      0.00      0.00         7\n","           3       0.00      0.00      0.00         7\n","           4       0.00      0.00      0.00         2\n","           5       0.00      0.00      0.00        14\n","           6       0.00      0.00      0.00         3\n","           7       0.00      0.00      0.00         9\n","           8       0.00      0.00      0.00        56\n","           9       0.00      0.00      0.00        10\n","          10       0.00      0.00      0.00         7\n","          11       0.00      0.00      0.00        11\n","          12       0.00      0.00      0.00         6\n","          13       0.00      0.00      0.00        14\n","          14       0.00      0.00      0.00        64\n","          15       0.00      0.00      0.00         4\n","          16       0.00      0.00      0.00         6\n","          17       0.00      0.00      0.00         8\n","          18       0.00      0.00      0.00         5\n","          19       0.00      0.00      0.00        37\n","          20       0.00      0.00      0.00         8\n","          21       0.00      0.00      0.00        47\n","          22       0.00      0.00      0.00        14\n","          23       0.00      0.00      0.00        26\n","          24       0.00      0.00      0.00       325\n","          25       0.00      0.00      0.00       150\n","          26       0.00      0.00      0.00       120\n","          27       0.00      0.00      0.00        62\n","          28       0.00      0.00      0.00       350\n","          29       0.00      0.00      0.00         9\n","          30       0.00      0.00      0.00        87\n","          31       0.33      1.00      0.50      1696\n","          32       0.00      0.00      0.00        19\n","          33       0.00      0.00      0.00        14\n","          34       0.00      0.00      0.00        10\n","          35       0.00      0.00      0.00        82\n","          36       0.00      0.00      0.00        82\n","          37       0.00      0.00      0.00       138\n","          38       0.00      0.00      0.00        99\n","          39       0.00      0.00      0.00        12\n","          40       0.00      0.00      0.00        15\n","          41       0.00      0.00      0.00        22\n","          42       0.00      0.00      0.00         9\n","          43       0.00      0.00      0.00        76\n","          44       0.00      0.00      0.00        10\n","          45       0.00      0.00      0.00        22\n","          46       0.00      0.00      0.00        60\n","          47       0.00      0.00      0.00        31\n","          48       0.00      0.00      0.00        36\n","          49       0.00      0.00      0.00         9\n","          50       0.00      0.00      0.00        51\n","          51       0.00      0.00      0.00        24\n","          52       0.00      0.00      0.00       229\n","          53       0.00      0.00      0.00        90\n","          54       0.00      0.00      0.00        71\n","          55       0.00      0.00      0.00        19\n","          56       0.00      0.00      0.00        25\n","          57       0.00      0.00      0.00        17\n","          58       0.00      0.00      0.00        31\n","          59       0.00      0.00      0.00        41\n","          60       0.00      0.00      0.00        53\n","          61       0.00      0.00      0.00        15\n","          62       0.00      0.00      0.00        65\n","          63       0.00      0.00      0.00        80\n","          64       0.00      0.00      0.00       110\n","          65       0.00      0.00      0.00        20\n","          66       0.00      0.00      0.00        75\n","          67       0.00      0.00      0.00        82\n","          68       0.00      0.00      0.00        14\n","          69       0.00      0.00      0.00         8\n","          70       0.00      0.00      0.00         9\n","          71       0.00      0.00      0.00        20\n","          72       0.00      0.00      0.00        11\n","          73       0.00      0.00      0.00        12\n","          74       0.00      0.00      0.00        13\n","          75       0.00      0.00      0.00         7\n","          76       0.00      0.00      0.00         2\n","\n","    accuracy                           0.33      5134\n","   macro avg       0.00      0.01      0.01      5134\n","weighted avg       0.11      0.33      0.16      5134\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["preds = models['logits'].detach().cpu().numpy()\n","preds = np.argmax(preds, axis = 1)\n","print(metrics.classification_report(test_y, preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADJcQXopgh49"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["JDSnLitbFrzd","5ZPKYmDPF3pJ","n3nTiLg-LdR8","eOdbVTRyLnx5","eY3fEsQJv1Vq","x9nPCEe_MA8V","itrudbMUNJHs","XxuSf8JqAhsw"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8159c023019644139af687d1512343e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c631c96878a478da6ce79399cbb5095","IPY_MODEL_653c364fed1f4d269754c8bd3a37f5ee","IPY_MODEL_f0ea8ff1338b480c9ca7f5f0898dfad1"],"layout":"IPY_MODEL_b4b04cc37a6348fd9c00ca6fe4f40580"}},"3c631c96878a478da6ce79399cbb5095":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3db0a8f0c45a4f27a358e2aac4d8d8e2","placeholder":"​","style":"IPY_MODEL_68a8bad307f54985ae032a3076551256","value":""}},"653c364fed1f4d269754c8bd3a37f5ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e91c4e6f42284ee6aeff00d896154519","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e413a3f9538a491591219857a66184f9","value":0}},"f0ea8ff1338b480c9ca7f5f0898dfad1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae42b0bcd6164425a3611431efc8158e","placeholder":"​","style":"IPY_MODEL_d60d06f04dc34ce8831d135130dcd14d","value":" 0/0 [00:00&lt;?, ?it/s]"}},"b4b04cc37a6348fd9c00ca6fe4f40580":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3db0a8f0c45a4f27a358e2aac4d8d8e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68a8bad307f54985ae032a3076551256":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e91c4e6f42284ee6aeff00d896154519":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e413a3f9538a491591219857a66184f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae42b0bcd6164425a3611431efc8158e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d60d06f04dc34ce8831d135130dcd14d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee543c72d7314fc9bc8dec6691612001":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1823470ecfa447fa1d868af66d77b52","IPY_MODEL_60d5e58dc0e44438aa39f5f96c887e99","IPY_MODEL_ca24abdaf11b49a59f59d962ece65e4d"],"layout":"IPY_MODEL_179b99aa78f64367aa3c757dbd4c23ae"}},"e1823470ecfa447fa1d868af66d77b52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc5fff94e55243c6a57639666e2580b4","placeholder":"​","style":"IPY_MODEL_4cd1583344f9478490e18c3f5a5ba798","value":"Downloading: 100%"}},"60d5e58dc0e44438aa39f5f96c887e99":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09a80f2c4c094d4c81c91b367ff88e52","max":810912,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6db6437e3de0446281646d278faf394d","value":810912}},"ca24abdaf11b49a59f59d962ece65e4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b3d57ce35374253aa229c2e3b00b7ee","placeholder":"​","style":"IPY_MODEL_86e91b31e8f542319d9ad55499fb5037","value":" 811k/811k [00:00&lt;00:00, 1.49MB/s]"}},"179b99aa78f64367aa3c757dbd4c23ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc5fff94e55243c6a57639666e2580b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cd1583344f9478490e18c3f5a5ba798":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09a80f2c4c094d4c81c91b367ff88e52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6db6437e3de0446281646d278faf394d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b3d57ce35374253aa229c2e3b00b7ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86e91b31e8f542319d9ad55499fb5037":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5bd3cc371294993829825d47009a2bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4be677ff599c4d6eaf104a3ada44f858","IPY_MODEL_ee970b3994f54db8bb4c94e63ea406aa","IPY_MODEL_b1173db4c4ac4b1386a2f62dbc86d552"],"layout":"IPY_MODEL_f1be8ef26a394ec5a1487126b33423f9"}},"4be677ff599c4d6eaf104a3ada44f858":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65ac6bd912614a13af485d35c4069bda","placeholder":"​","style":"IPY_MODEL_383e5f8e7444452eba1f5d59aa18e87c","value":"Downloading: 100%"}},"ee970b3994f54db8bb4c94e63ea406aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f700458ad5e147eb974be7c419880698","max":508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4ee898f344c4adb90bb5b42aa1310f8","value":508}},"b1173db4c4ac4b1386a2f62dbc86d552":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e47316a128645e28ee722870f02e6b5","placeholder":"​","style":"IPY_MODEL_4114439fdf9c4ef29ab72b5931affa9d","value":" 508/508 [00:00&lt;00:00, 14.5kB/s]"}},"f1be8ef26a394ec5a1487126b33423f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65ac6bd912614a13af485d35c4069bda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"383e5f8e7444452eba1f5d59aa18e87c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f700458ad5e147eb974be7c419880698":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4ee898f344c4adb90bb5b42aa1310f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2e47316a128645e28ee722870f02e6b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4114439fdf9c4ef29ab72b5931affa9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6ca55f53312472f8694597aecb8cebb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb1e5856c46c4c989b0e268642a059a8","IPY_MODEL_e8b5c2d856744bc5b43f3da83e8c994c","IPY_MODEL_e2d13acb02ff449896b020e8e850ee41"],"layout":"IPY_MODEL_1b984e85f71c4a82ae4a8598c74e56b5"}},"eb1e5856c46c4c989b0e268642a059a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1a68827591f463e9ad448393548bdba","placeholder":"​","style":"IPY_MODEL_af04478534674808941cc2db64456fef","value":"Downloading: 100%"}},"e8b5c2d856744bc5b43f3da83e8c994c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14fcbf82be2f478b866272f3019395fa","max":445032417,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3368e92c19d542b7b0c4aef38f06069b","value":445032417}},"e2d13acb02ff449896b020e8e850ee41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_855fa8e8b1804b7a825b7a0409c86eb6","placeholder":"​","style":"IPY_MODEL_3720e112425b490b9dbf6f9e70158ddd","value":" 445M/445M [00:07&lt;00:00, 58.5MB/s]"}},"1b984e85f71c4a82ae4a8598c74e56b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1a68827591f463e9ad448393548bdba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af04478534674808941cc2db64456fef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14fcbf82be2f478b866272f3019395fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3368e92c19d542b7b0c4aef38f06069b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"855fa8e8b1804b7a825b7a0409c86eb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3720e112425b490b9dbf6f9e70158ddd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}